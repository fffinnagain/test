{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regular scrapper for the fanworks in FFnet. Collects metadata on unrestricted works listed for a search query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) load the relevant python libraries\n",
    "2) define the scraping function for query pages\n",
    "3) set up the database name, the search query url, and the hold time 3-5 s\n",
    "4) create the fanwork database (in MySQL)\n",
    "*5a) If the script is interrupted, call on the last page queried with the variable below (uncommented) *\n",
    "5) test the scrapper on FFnet query page,\n",
    "6) populate the database from FFnet \n",
    "    this runs until it brakes because of page laoding issues or it reaches the end of the list of works.\n",
    "7) check the status of the database and scraping \n",
    "8) cleaning up duplicates in a database\n",
    "\n",
    "And that's it. Now you have a database of metadata on fanworks on FFnet according to some search queury, like Sherlock/Harry Potter Crossovers, or Agent Carter (TV), or anything you like, really. I wouldn't try downloading the whole archive, that's just rude, but otherwise, go to town."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The query structure for FFnet is fits to the metadata presented.\n",
    "\n",
    "The strapper collects the metadata available from the information listed on FFnet query pages. For each work listed, the database retains:\n",
    "\n",
    "1) 'url': String\n",
    "\n",
    "2) 'Node': Int\n",
    "\n",
    "3) 'Title': String\n",
    "\n",
    "4) 'Creator': String\n",
    "\n",
    "5) 'Fandoms': List of strings\n",
    "\n",
    "6) 'Summary': String\n",
    "\n",
    "7)  'Rating': String {K, K+, T, M}\n",
    "\n",
    "8) 'Language': String\n",
    "\n",
    "9) 'Genre': List of strings\n",
    "\n",
    "10) 'Relationships': List of strings\n",
    "\n",
    "11) 'Characters': List of strings\n",
    "\n",
    "12) 'Chapters': Int\n",
    "\n",
    "13) 'Words': Int\n",
    "\n",
    "14) 'Reviews': Int\n",
    "\n",
    "15) 'Favs': Int\n",
    "\n",
    "16) 'Follows': Int\n",
    "\n",
    "17) 'Update': Int (YYYYMMDD, date last updated)\n",
    "\n",
    "18) 'Published': Int (YYYYMMDD, date first published)\n",
    "\n",
    "19) 'Complete': Int {1 for complete, 0 for Incomplete}\n",
    "\n",
    "Some feilds may be empty of they are not reported on the query page, as, for example, FFnet doesn't report Relationships when the creator doesn't specify.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1) load the relevant python libraries\n",
    "\n",
    "# necessary libraries. One day I will use beautiful soup.\n",
    "import requests\n",
    "import lxml\n",
    "import time\n",
    "from lxml import html\n",
    "from dumptruck import DumpTruck\n",
    "from time import strptime\n",
    "from datetime import datetime\n",
    "from datetime import date\n",
    "from collections import OrderedDict\n",
    "\n",
    "from collections import Counter\n",
    "from collections import deque\n",
    "import math\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 2) define the scraping function for query pages\n",
    "\n",
    "def query_FFN_work(titles,wurls,summaries,stats,times,fandoms,today):\n",
    "    \n",
    "    entry = {'url': '', \n",
    "             'Node': 0, \n",
    "             'Title': '', \n",
    "             'Creator': '', \n",
    "             'Fandoms': [fandoms], \n",
    "             'Summary': '' , \n",
    "             'Rating': '', \n",
    "             'Language': '' , \n",
    "             'Genre': [], \n",
    "             'Relationships': [], \n",
    "             'Characters': [],\n",
    "             'Chapters': 0, \n",
    "             'Words': 0, \n",
    "             'Reviews': 0, \n",
    "             'Favs': 0, \n",
    "             'Follows': 0, \n",
    "             'Updated': 0, \n",
    "             'Published': 0,  \n",
    "             'Complete': 0}\n",
    "    \n",
    "    if len(wurls)>0:\n",
    "        if titles[0] == 'reviews':\n",
    "            titles.popleft()\n",
    "        entry['Title'] = titles.popleft()\n",
    "        entry['Creator'] = titles.popleft()\n",
    "        entry['url'] = wurls.popleft()\n",
    "        entry['Node'] = entry['url'].split('/')[4]\n",
    "        entry['Summary'] = summaries.popleft()\n",
    "\n",
    "        ## stats and times\n",
    "        stat = stats.popleft()\n",
    "        while len(stats)> 0 and stats[0][:5] != 'Rated':\n",
    "            stat += stats.popleft()\n",
    "        details = deque(stat.split(' - '))\n",
    "        #print details\n",
    "\n",
    "        if details[-1] == 'Complete':\n",
    "            entry['Complete'] = 1\n",
    "            details.pop()\n",
    "        if details[0][:5] == 'Rated':\n",
    "            r = details.popleft().split(': ')\n",
    "            entry['Rating'] = r[-1]\n",
    "            entry['Language'] = details.popleft()\n",
    "        if details[0][:4] != 'Chap':\n",
    "            genre = details.popleft().split('/')\n",
    "            for g in genre:\n",
    "                if g == 'Comfort': # arg Hurt/Comfort\n",
    "                    entry['Genre'][-1] += '/'\n",
    "                    entry['Genre'][-1] += g\n",
    "                else:\n",
    "                    entry['Genre'].append(g)\n",
    "        if details[0][:4] == 'Chap':\n",
    "            entry['Chapters'] = int(details.popleft().split(': ')[-1])\n",
    "        if details[0][:4] == 'Word':\n",
    "            dig = details.popleft().split(': ')[-1].split(',')\n",
    "            if len(dig) == 1:\n",
    "                numb = int(dig[0])\n",
    "            else:  \n",
    "                numb = 0\n",
    "                i = 0\n",
    "                while len(dig)>0:\n",
    "                    numb += int(dig.pop())*math.pow(1000, i)\n",
    "                    i+=1\n",
    "                numb = int(numb)\n",
    "            entry['Words'] = numb\n",
    "        if details[0][:4] == 'Revi':\n",
    "            dig = details.popleft().split(': ')[-1].split(',')\n",
    "            if len(dig) == 1:\n",
    "                numb = int(dig[0])\n",
    "            else:  \n",
    "                numb = 0\n",
    "                i = 0\n",
    "                while len(dig)>0:\n",
    "                    numb += int(dig.pop())*math.pow(1000, i)\n",
    "                    i+=1\n",
    "                numb = int(numb)\n",
    "            entry['Reviews'] = numb\n",
    "        if details[0][:4] == 'Favs':\n",
    "            dig = details.popleft().split(': ')[-1].split(',')\n",
    "            if len(dig) == 1:\n",
    "                numb = int(dig[0])\n",
    "            else:    \n",
    "                numb = 0\n",
    "                i = 0\n",
    "                while len(dig)>0:\n",
    "                    numb += int(dig.pop())*math.pow(1000, i)\n",
    "                    i+=1\n",
    "                numb = int(numb)\n",
    "            entry['Favs'] = numb\n",
    "        if details[0][:4] == 'Foll':\n",
    "            dig = details.popleft().split(': ')[-1].split(',')\n",
    "            if len(dig) == 1:\n",
    "                numb = int(dig[0])\n",
    "            else:  \n",
    "                numb = 0\n",
    "                i = 0\n",
    "                while len(dig)>0:\n",
    "                    numb += int(dig.pop())*math.pow(1000, i)\n",
    "                    i+=1\n",
    "                numb = int(numb)\n",
    "            entry['Follows'] = numb\n",
    "            \n",
    "        if details[0] == 'Updated: ':\n",
    "        #if details[0][:4] == 'Upda':\n",
    "            details.popleft()\n",
    "            t = times.popleft()\n",
    "            t = t.split('/')\n",
    "            if len(t) < 2:\n",
    "                entry['Updated'] = today\n",
    "            else: \n",
    "                if len(t) < 3:\n",
    "                    year = today/10000\n",
    "                    entry['Updated'] = year + int(t[0])*100 + int(t[1])\n",
    "                else:\n",
    "                    entry['Updated'] = int(t[2])*10000 + int(t[0])*100 + int(t[1])\n",
    "\n",
    "        #if details[0][:4] == 'Publ':\n",
    "        if details[0] =='Published: ':\n",
    "            details.popleft()\n",
    "            t = times.popleft()\n",
    "            t = t.split('/')\n",
    "            if len(t) < 2:\n",
    "                entry['Published'] = today\n",
    "            else: \n",
    "                if len(t) < 3:\n",
    "                    year = today/10000\n",
    "                    entry['Published'] = int(year)*10000 + int(t[0])*100 + int(t[1])\n",
    "                else:\n",
    "                    entry['Published'] = int(t[2])*10000 + int(t[0])*100 + int(t[1])\n",
    "\n",
    "        if entry['Updated'] == 0:\n",
    "            entry['Updated'] = entry['Published']\n",
    "#         if entry['Published'] == 0:\n",
    "#             entry['Published'] = entry['Updated']\n",
    "\n",
    "        if len(details) == 1:\n",
    "            r = details.popleft().split(']')\n",
    "            #print r\n",
    "            for rel in r:\n",
    "                if len(rel)>0:\n",
    "                    if rel[0] == '[':\n",
    "                        rel = rel[1:]\n",
    "                        entry['Relationships'].append(rel)\n",
    "                    entry['Characters'] += rel.split(', ')\n",
    "                #print entry['Characters']\n",
    "\n",
    "        # check for parsing failures \n",
    "        if len(details)!= 0:\n",
    "            print 'Failed parse'\n",
    "            print details\n",
    "            print entry['Node']\n",
    "        #print entry    \n",
    "        \n",
    "    return [entry,titles,wurls,summaries,stats,times]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 3) set up the database name, the search query url, and the hold interval (3 seconds is recommended)\n",
    "\n",
    "#Specify database name\n",
    "database_name = \"FFN_StarTrek-Voyager\"\n",
    "\n",
    "url = 'https://www.fanfiction.net/tv/StarTrek-Voyager/?&srt=1&r=10'\n",
    "# specify pause time between page quaries, in seconds.\n",
    "hold = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 4) create the fanwork database (in MySQL)\n",
    "\n",
    "#set up data base, using current date to specify database\n",
    "TODAY = datetime.today().isoformat()\n",
    "today = int(TODAY[0:4])*10000+int(TODAY[5:7])*100+int(TODAY[8:10])\n",
    "fandoms = 'StarTrek: Voyager'\n",
    "\n",
    "# not sure if all this is necessary\n",
    "LAST = 0\n",
    "PAGES = 363\n",
    "pages = 0\n",
    "lastNode = 12489605 # from most recent work?\n",
    "\n",
    "DBname = database_name + \"_\" + TODAY[:10] + \".db\"\n",
    "print DBname\n",
    "#Example: DBname = 'AO3_Supernatural_2016-05-02.db'\n",
    "\n",
    "dt = DumpTruck(dbname=DBname)\n",
    "if 'dumptruck' in dt.tables():\n",
    "    data = dt.dump()\n",
    "    print 'Adding to existing database'\n",
    "    print len(data)\n",
    "    print data[0]\n",
    "    print data[-1]\n",
    "    # dt.drop()\n",
    "else:\n",
    "    print 'Generating new database'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 5a) If the script is interrupted, call on the last page queried with the variable below (uncommented) \n",
    "\n",
    "#url = 'https://www.fanfiction.net/tv/Sherlock/?&srt=2&r=10&p=2003'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 5) test the scrapper on FFnet query page,\n",
    "\n",
    "# access the query page and extract the list of max 25 works \n",
    "page = requests.get(url)\n",
    "tree = html.fromstring(page.text)\n",
    "tree.make_links_absolute(url)\n",
    "\n",
    "works = tree.xpath('//div[@class=\"z-list zhover zpointer \"]')\n",
    "if len(works) < 1:\n",
    "    print url\n",
    "    time.sleep(hold)\n",
    "    page = requests.get(url)\n",
    "    tree = html.fromstring(page.text)\n",
    "    tree.make_links_absolute(url)\n",
    "    works = tree.xpath('//div[@class=\"z-list zhover zpointer \"]')\n",
    "\n",
    "titles = deque(works[0].xpath('//div[@class=\"z-list zhover zpointer \"]/a/text()'))\n",
    "wurls = deque(works[0].xpath('//a[@class=\"stitle\"]/@href'))\n",
    "summaries = deque(works[0].xpath('//div[@class =\"z-indent z-padtop\"]/text()'))\n",
    "stats = deque(works[0].xpath('//div[@class =\"z-padtop2 xgray\"]/text()'))\n",
    "times = deque(works[0].xpath('//div[@class =\"z-padtop2 xgray\"]/span/text()'))\n",
    "# print times\n",
    "# print stats\n",
    "\n",
    "links = works[0].xpath('//center[@style=\"margin-top:5px;margin-bottom:5px;\"]/a/@href')\n",
    "nextURL = links[-1]\n",
    "print nextURL\n",
    "\n",
    "while len(wurls)>20:\n",
    "    [item,titles,wurls,summaries,stats,times] = query_FFN_work(titles,wurls,summaries,stats,times,fandoms,today)    \n",
    "    print item\n",
    "    dt.insert(item)\n",
    "# print times\n",
    "# print stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 6) populate the database from FFnet\n",
    "LAST = 0\n",
    "while LAST < 1:\n",
    "    page = requests.get(nextURL)\n",
    "    tree = html.fromstring(page.text)\n",
    "    tree.make_links_absolute(nextURL)\n",
    "\n",
    "    works = tree.xpath('//div[@class=\"z-list zhover zpointer \"]')\n",
    "    if len(works) < 1:\n",
    "        print nextURL\n",
    "        time.sleep(4)\n",
    "        page = requests.get(nextURL)\n",
    "        tree = html.fromstring(page.text)\n",
    "        tree.make_links_absolute(nextURL)\n",
    "        works = tree.xpath('//div[@class=\"z-list zhover zpointer \"]')\n",
    "        \n",
    "    titles = deque(works[0].xpath('//div[@class=\"z-list zhover zpointer \"]/a/text()'))\n",
    "    wurls = deque(works[0].xpath('//a[@class=\"stitle\"]/@href'))\n",
    "    summaries = deque(works[0].xpath('//div[@class =\"z-indent z-padtop\"]/text()'))\n",
    "    stats = deque(works[0].xpath('//div[@class =\"z-padtop2 xgray\"]/text()'))\n",
    "    times = deque(works[0].xpath('//div[@class =\"z-padtop2 xgray\"]/span/text()'))\n",
    "\n",
    "    while len(wurls)>0:\n",
    "        [item,titles,wurls,summaries,stats,times] = query_FFN_work(titles,wurls,summaries,stats,times,fandoms,today)    \n",
    "        dt.insert(item)\n",
    "        \n",
    "    # call the next page, if we haven't reached the end of PAGES or search\n",
    "    links = works[0].xpath('//center[@style=\"margin-top:5px;margin-bottom:5px;\"]/a/@href')\n",
    "    if pages < PAGES and int(links[-2].split('=')[-1])>int(links[-1].split('=')[-1]):\n",
    "        nextURL = links[-1]\n",
    "        print nextURL\n",
    "        pages +=1\n",
    "    else:\n",
    "        LAST = 1\n",
    "    \n",
    "    lastNode = item['Node']\n",
    "    print lastNode\n",
    "        \n",
    "    # pause before next line\n",
    "    time.sleep(hold)\n",
    "    \n",
    "print 'Finnished run through archive.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# if for whatever reason it's necessary to limit the number of query pages accessed...\n",
    "\n",
    "PAGES = 2295 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 7) check the status of the database and scraping \n",
    "\n",
    "data = dt.dump()\n",
    "print url\n",
    "print data[-1]\n",
    "print LAST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 8) cleaning up duplicates in a database\n",
    "DBname = DBname[:-3]+\"_cleaned.db\"\n",
    "cleanDB = DumpTruck(dbname=DBname)\n",
    "if 'dumptruck' in cleanDB.tables():\n",
    "    cleanDB.drop() # nothing to see here\n",
    "#data = dt.dump() # data to be cleaned of duplicates\n",
    "NODES = set() # numbers \n",
    "\n",
    "# work in data, check NODE against NODES list\n",
    "# if the node is not already there, insert it and insert work into clearDB\n",
    "\n",
    "for work in data:\n",
    "    if work['Node'] not in NODES:\n",
    "        cleanDB.insert(work)\n",
    "        NODES.add(work['Node'])\n",
    "\n",
    "print len(data)\n",
    "cleandata = cleanDB.dump()\n",
    "print len(cleandata)\n",
    "print len(NODES)\n",
    "print DBname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
